{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "In this notebook I will implement the machine learning algorithms described in the course videos. That is, Naive Bayes, Naive Bayes with Laplace smoothing, one dimensional linear regression, logistic regression, regularized regression by gradient descent, perceptron, K-nearest neighbours, Gaussian mixture models, and possibly some eigenvector methods for dimension reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised Learning\n",
    "##### Naive Bayes - Classification\n",
    "Here we have a data set $X$ where each $x_i \\in X$ has a class $y_i \\in Y$. For simplicity we assume $|Y| = 2$, i.e. binary classification. Further each $x_i$ is a sequence $x_i = \\{w_{i,j}\\}_{j =1} ^{m_i}$, and we make the naive independence assumption that\n",
    "\n",
    "$$ P(x_i | y_i) = \\Pi_{j=1} ^{m_i} P(w_{i,j} |y_i). $$\n",
    "\n",
    "Then given a new data point $x = \\{w_j\\}_{j=1} ^m$ we can compute the probability of assigning class $y$ to $x$ by Bayes rule\n",
    "\n",
    "$$ P(y|x) = \\frac{P(x|y)P(y)}{P(x)} = \\frac{P(y) \\Pi_{j=1} ^m P(w_j |y)}{\\sum_{\\hat{y} \\in Y} P(\\hat{y})\\Pi_{j=1} ^m P(w_j | \\hat{y})} $$\n",
    "\n",
    "We can approximate the priors $P(y)$ for each $y \\in Y$ by simply the fraction of $x_i$ with class $y$. The conditionals $P(w|y)$ can be computed as the frequency $w$ appears in all $x_i$ in class $y$. These are the emperical/frequentist estimates. \n",
    "\n",
    "The standard example of this is spam detection of emails, so I will name things with that in mind, however the method should work in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_punctuation(text):\n",
    "    import string\n",
    "    punc = set(string.punctuation)\n",
    "    return (''.join(ch for ch in text if ch not in punc))\n",
    "\n",
    "def process_text(text):\n",
    "    return strip_punctuation(text).lower()\n",
    "\n",
    "class Email:\n",
    "        \n",
    "    def __init__(self, text, spam):\n",
    "        self.text = text\n",
    "        self.spam = spam\n",
    "        \n",
    "def email_to_words(email):\n",
    "    return process_text(email.text).split()\n",
    "        \n",
    "def get_vocabulary(emails):\n",
    "    vocab = set()\n",
    "    for email in emails:\n",
    "        words = email_to_words(email)\n",
    "        for word in words:\n",
    "            vocab.add(word)\n",
    "    return vocab\n",
    "\n",
    "def get_priors(emails):\n",
    "    priors = {\"spam\":0, \"ham\":0}\n",
    "    for email in emails:\n",
    "        if email.spam: priors['spam']+=1\n",
    "        else : priors['ham']+=1\n",
    "    for key in priors:\n",
    "        priors[key] /= len(emails)\n",
    "    return priors\n",
    "\n",
    "def get_word_conditionals(emails):\n",
    "    vocab = get_vocabulary(emails)\n",
    "    spam_conditionals = {}\n",
    "    num_spam = 0\n",
    "    ham_conditionals = {}\n",
    "    num_ham = 0\n",
    "    for email in emails:\n",
    "        words = email_to_words(email)\n",
    "        for word in words:\n",
    "            if email.spam:\n",
    "                num_spam+=1\n",
    "                if word in spam_conditionals: spam_conditionals[word]+=1\n",
    "                else : spam_conditionals[word]=1\n",
    "            else:\n",
    "                num_ham+=1\n",
    "                if word in ham_conditionals : ham_conditionals[word]+=1\n",
    "                else : ham_conditionals[word] = 1\n",
    "    conditionals = [spam_conditionals, ham_conditionals]\n",
    "    nums = [num_spam, num_ham]\n",
    "    for conditional, num in zip(conditionals,nums):\n",
    "        for word in vocab:\n",
    "            if word not in conditional: conditional[word]=0\n",
    "            else : conditional[word] = conditional[word]/num\n",
    "    return spam_conditionals, ham_conditionals\n",
    "\n",
    "def naive_bayes_classification(new_email, emails):\n",
    "    spam_conditionals, ham_conditionals = get_word_conditionals(emails)\n",
    "    priors = get_priors(emails)\n",
    "    numerator = 1\n",
    "    denominator = 1\n",
    "    words = email_to_words(new_email)\n",
    "    for word in words:\n",
    "        numerator*=spam_conditionals[word]\n",
    "        denominator*=ham_conditionals[word]\n",
    "    numerator*=priors['spam']\n",
    "    denominator = denominator*priors['ham']+numerator\n",
    "    return numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ham': 0.625, 'spam': 0.375}\n"
     ]
    }
   ],
   "source": [
    "# example given in videos\n",
    "email_text = [\"offer is secret\", \"click secret link\", \"secret sports link\", \n",
    "         \"play sports today\", \"went play sports\", \"secret sports event\", \n",
    "         \"sports is today\", \"sports costs money\"]\n",
    "spam_labels = [True,True,True, False,False,False,False,False]\n",
    "emails = []\n",
    "for text,label in zip(email_text,spam_labels):\n",
    "    emails.append(Email(text,label))\n",
    "\n",
    "priors = get_priors(emails)\n",
    "print(priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'secret': 0.3333333333333333, 'event': 0, 'went': 0, 'link': 0.2222222222222222, 'offer': 0.1111111111111111, 'money': 0, 'today': 0, 'play': 0, 'is': 0.1111111111111111, 'costs': 0, 'click': 0.1111111111111111, 'sports': 0.1111111111111111}\n"
     ]
    }
   ],
   "source": [
    "spam_conditionals, _ = get_word_conditionals(emails)\n",
    "print(spam_conditionals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of \"spORts\" being spam: 0.16666666666666669\n",
      "Probability of \"sec'ret i@s secr~et\" being spam: 0.9615384615384616\n",
      "Probability of \"ToDA&y Is .secret\" being spam: 0.0\n"
     ]
    }
   ],
   "source": [
    "# examples from videos, with added punctuation and case noise\n",
    "new_email_text = [\"spORts\", \"sec'ret i@s secr~et\", \"ToDA&y Is .secret\"]\n",
    "for text in new_email_text:\n",
    "    new_email = Email(text, None)\n",
    "    print(\"Probability of \\\"{}\\\" being spam: {}\".format(text, naive_bayes_classification(new_email, emails)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final example shows why we might want to use a regularisation method such as Laplace smoothing. The word \"today\" never appears in the spam data set, so $P(today | spam) = 0$ according to the frequentist approximation. \n",
    "\n",
    "Laplace smoothing is parameterised by a positive integer $k$. Given a random variable $X$ that takes values $x_1, \\ldots, x_m$, suppose $X_1, \\ldots, X_n$ are samples of $X$. Then to estimate $P(X=x_i)$ we use the Laplace smoothing\n",
    "\n",
    "$$ P(X=x_i) \\approx \\frac{k + \\sum_{j=1} ^n \\chi(X_j = x_i)}{km + n} $$\n",
    "\n",
    "In the case of emails, $x_i$ denote all the possible words, and $X_j$ are the words we find in a dataset. The smoothing can also be applied to the priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_laplace_priors(emails, k):\n",
    "    priors = {\"spam\":0, \"ham\":0}\n",
    "    for email in emails:\n",
    "        if email.spam: priors['spam']+=1\n",
    "        else : priors['ham']+=1\n",
    "    for key in priors:\n",
    "        priors[key]+=k\n",
    "        priors[key] /= (len(emails)+2*k)\n",
    "    return priors\n",
    "\n",
    "def get_laplace_word_conditionals(emails, k):\n",
    "    vocab = get_vocabulary(emails)\n",
    "    spam_conditionals = {}\n",
    "    num_spam = 0\n",
    "    ham_conditionals = {}\n",
    "    num_ham = 0\n",
    "    for email in emails:\n",
    "        words = email_to_words(email)\n",
    "        for word in words:\n",
    "            if email.spam:\n",
    "                num_spam+=1\n",
    "                if word in spam_conditionals: spam_conditionals[word]+=1\n",
    "                else : spam_conditionals[word]=1\n",
    "            else:\n",
    "                num_ham+=1\n",
    "                if word in ham_conditionals : ham_conditionals[word]+=1\n",
    "                else : ham_conditionals[word] = 1\n",
    "    conditionals = [spam_conditionals, ham_conditionals]\n",
    "    nums = [num_spam, num_ham]\n",
    "    for conditional, num in zip(conditionals,nums):\n",
    "        for word in vocab:\n",
    "            if word not in conditional: conditional[word]=k\n",
    "            else : conditional[word] = (conditional[word]+k)/(num+k*len(vocab))\n",
    "    return spam_conditionals, ham_conditionals\n",
    "\n",
    "def laplace_naive_bayes_classification(new_email, emails,k):\n",
    "    spam_conditionals, ham_conditionals = get_laplace_word_conditionals(emails,k)\n",
    "    priors = get_laplace_priors(emails,k)\n",
    "    numerator = 1\n",
    "    denominator = 1\n",
    "    words = email_to_words(new_email)\n",
    "    for word in words:\n",
    "        numerator*=spam_conditionals[word]\n",
    "        denominator*=ham_conditionals[word]\n",
    "    numerator*=priors['spam']\n",
    "    denominator = denominator*priors['ham']+numerator\n",
    "    return numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of \"spORts\" being spam: 0.22222222222222224\n",
      "Probability of \"sec'ret i@s secr~et\" being spam: 0.85002186270223\n",
      "Probability of \"ToDA&y Is .secret\" being spam: 0.9520078354554358\n"
     ]
    }
   ],
   "source": [
    "# examples from videos, with added punctuation and case noise\n",
    "new_email_text = [\"spORts\", \"sec'ret i@s secr~et\", \"ToDA&y Is .secret\"]\n",
    "for text in new_email_text:\n",
    "    new_email = Email(text, None)\n",
    "    print(\"Probability of \\\"{}\\\" being spam: {}\".format(text, laplace_naive_bayes_classification(new_email, emails,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Dimensional Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
